{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import sys\n",
    "sys.path.insert(1, '../scripts/')\n",
    "import data_munging_tools as dmt\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframes\n",
    "test_df = pd.read_csv('../data/cleaned-input.test.tsv', sep='\\t', low_memory=False)\n",
    "train_df = pd.read_csv('../data/cleaned-input.training.tsv', sep='\\t', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "blacklist_patterns = ['^recent_ipt_', '^production_', 'total_num_stages', 'bakken_isopach_ft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1586, 53)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitelist = ['production_liquid_180']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FileNo', 'CountyName', 'CurrentOperator', 'CurrentWellName', 'DFElev',\n",
       "       'FieldName', 'Footages', 'GRElev', 'KBElev', 'LeaseName', 'LeaseNumber',\n",
       "       'OriginalOperator', 'OriginalWellName', 'ProducedPools', 'QQ', 'Range',\n",
       "       'Section', 'TD', 'Township', 'WellStatus', 'WellType', 'Wellbore',\n",
       "       'api', 'bakken_isopach_ft', 'bh_lat', 'bh_lng', 'choke_size', 'legs',\n",
       "       'max_tvd', 'mean_tvd', 'min_tvd', 'num_pools_produced',\n",
       "       'production_liquid_120', 'production_liquid_150',\n",
       "       'production_liquid_180', 'production_liquid_1825',\n",
       "       'production_liquid_270', 'production_liquid_30',\n",
       "       'production_liquid_365', 'production_liquid_60',\n",
       "       'production_liquid_730', 'production_liquid_90', 'spud_date', 'std_tvd',\n",
       "       'stimulated_formation', 'surface_lat', 'surface_lng',\n",
       "       'total_lbs_proppant', 'total_num_stages', 'total_volume_bbls', 'tvd',\n",
       "       'type_treatment', 'well_status_date'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_missing</th>\n",
       "      <th>perc_missing</th>\n",
       "      <th>num_zero</th>\n",
       "      <th>perc_zero</th>\n",
       "      <th>num_neg</th>\n",
       "      <th>perc_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FileNo</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CountyName</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CurrentOperator</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CurrentWellName</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DFElev</th>\n",
       "      <td>6527.0</td>\n",
       "      <td>99.97</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FieldName</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Footages</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GRElev</th>\n",
       "      <td>1138.0</td>\n",
       "      <td>17.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KBElev</th>\n",
       "      <td>393.0</td>\n",
       "      <td>6.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeaseName</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeaseNumber</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OriginalOperator</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OriginalWellName</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ProducedPools</th>\n",
       "      <td>11.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QQ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Range</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Section</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TD</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Township</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WellStatus</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WellType</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wellbore</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>api</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bakken_isopach_ft</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bh_lat</th>\n",
       "      <td>333.0</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bh_lng</th>\n",
       "      <td>333.0</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6196.0</td>\n",
       "      <td>94.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>choke_size</th>\n",
       "      <td>623.0</td>\n",
       "      <td>9.54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>legs</th>\n",
       "      <td>333.0</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_tvd</th>\n",
       "      <td>333.0</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_tvd</th>\n",
       "      <td>333.0</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min_tvd</th>\n",
       "      <td>333.0</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_pools_produced</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>production_liquid_120</th>\n",
       "      <td>143.0</td>\n",
       "      <td>2.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>production_liquid_150</th>\n",
       "      <td>165.0</td>\n",
       "      <td>2.53</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>production_liquid_180</th>\n",
       "      <td>224.0</td>\n",
       "      <td>3.43</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>production_liquid_1825</th>\n",
       "      <td>5860.0</td>\n",
       "      <td>89.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>production_liquid_270</th>\n",
       "      <td>592.0</td>\n",
       "      <td>9.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>production_liquid_30</th>\n",
       "      <td>109.0</td>\n",
       "      <td>1.67</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>production_liquid_365</th>\n",
       "      <td>1065.0</td>\n",
       "      <td>16.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>production_liquid_60</th>\n",
       "      <td>122.0</td>\n",
       "      <td>1.87</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>production_liquid_730</th>\n",
       "      <td>2789.0</td>\n",
       "      <td>42.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>production_liquid_90</th>\n",
       "      <td>133.0</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spud_date</th>\n",
       "      <td>26.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std_tvd</th>\n",
       "      <td>333.0</td>\n",
       "      <td>5.10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stimulated_formation</th>\n",
       "      <td>816.0</td>\n",
       "      <td>12.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface_lat</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surface_lng</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6529.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_lbs_proppant</th>\n",
       "      <td>871.0</td>\n",
       "      <td>13.34</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_num_stages</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_volume_bbls</th>\n",
       "      <td>984.0</td>\n",
       "      <td>15.07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tvd</th>\n",
       "      <td>333.0</td>\n",
       "      <td>5.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type_treatment</th>\n",
       "      <td>1122.0</td>\n",
       "      <td>17.18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>well_status_date</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        num_missing  perc_missing  num_zero  perc_zero  \\\n",
       "FileNo                          0.0          0.00       0.0       0.00   \n",
       "CountyName                      0.0          0.00       NaN        NaN   \n",
       "CurrentOperator                 0.0          0.00       NaN        NaN   \n",
       "CurrentWellName                 0.0          0.00       NaN        NaN   \n",
       "DFElev                       6527.0         99.97       0.0       0.00   \n",
       "FieldName                       0.0          0.00       NaN        NaN   \n",
       "Footages                        0.0          0.00       NaN        NaN   \n",
       "GRElev                       1138.0         17.43       0.0       0.00   \n",
       "KBElev                        393.0          6.02       0.0       0.00   \n",
       "LeaseName                       0.0          0.00       NaN        NaN   \n",
       "LeaseNumber                     3.0          0.05       NaN        NaN   \n",
       "OriginalOperator                0.0          0.00       NaN        NaN   \n",
       "OriginalWellName                0.0          0.00       NaN        NaN   \n",
       "ProducedPools                  11.0          0.17       NaN        NaN   \n",
       "QQ                              0.0          0.00       NaN        NaN   \n",
       "Range                           0.0          0.00       NaN        NaN   \n",
       "Section                         0.0          0.00       0.0       0.00   \n",
       "TD                             14.0          0.21       0.0       0.00   \n",
       "Township                        0.0          0.00       NaN        NaN   \n",
       "WellStatus                      0.0          0.00       NaN        NaN   \n",
       "WellType                        0.0          0.00       NaN        NaN   \n",
       "Wellbore                        0.0          0.00       NaN        NaN   \n",
       "api                             0.0          0.00       NaN        NaN   \n",
       "bakken_isopach_ft               0.0          0.00       0.0       0.00   \n",
       "bh_lat                        333.0          5.10       0.0       0.00   \n",
       "bh_lng                        333.0          5.10       0.0       0.00   \n",
       "choke_size                    623.0          9.54       NaN        NaN   \n",
       "legs                          333.0          5.10       0.0       0.00   \n",
       "max_tvd                       333.0          5.10       0.0       0.00   \n",
       "mean_tvd                      333.0          5.10       0.0       0.00   \n",
       "min_tvd                       333.0          5.10       0.0       0.00   \n",
       "num_pools_produced              0.0          0.00       0.0       0.00   \n",
       "production_liquid_120         143.0          2.19       0.0       0.00   \n",
       "production_liquid_150         165.0          2.53       0.0       0.00   \n",
       "production_liquid_180         224.0          3.43       0.0       0.00   \n",
       "production_liquid_1825       5860.0         89.75       0.0       0.00   \n",
       "production_liquid_270         592.0          9.07       0.0       0.00   \n",
       "production_liquid_30          109.0          1.67      18.0       0.28   \n",
       "production_liquid_365        1065.0         16.31       0.0       0.00   \n",
       "production_liquid_60          122.0          1.87       2.0       0.03   \n",
       "production_liquid_730        2789.0         42.72       0.0       0.00   \n",
       "production_liquid_90          133.0          2.04       0.0       0.00   \n",
       "spud_date                      26.0          0.40       NaN        NaN   \n",
       "std_tvd                       333.0          5.10       1.0       0.02   \n",
       "stimulated_formation          816.0         12.50       NaN        NaN   \n",
       "surface_lat                     0.0          0.00       0.0       0.00   \n",
       "surface_lng                     0.0          0.00       0.0       0.00   \n",
       "total_lbs_proppant            871.0         13.34       2.0       0.03   \n",
       "total_num_stages                0.0          0.00       0.0       0.00   \n",
       "total_volume_bbls             984.0         15.07       1.0       0.02   \n",
       "tvd                           333.0          5.10       0.0       0.00   \n",
       "type_treatment               1122.0         17.18       NaN        NaN   \n",
       "well_status_date                0.0          0.00       NaN        NaN   \n",
       "\n",
       "                        num_neg  perc_neg  \n",
       "FileNo                      0.0       0.0  \n",
       "CountyName                  NaN       NaN  \n",
       "CurrentOperator             NaN       NaN  \n",
       "CurrentWellName             NaN       NaN  \n",
       "DFElev                      0.0       0.0  \n",
       "FieldName                   NaN       NaN  \n",
       "Footages                    NaN       NaN  \n",
       "GRElev                      0.0       0.0  \n",
       "KBElev                      0.0       0.0  \n",
       "LeaseName                   NaN       NaN  \n",
       "LeaseNumber                 NaN       NaN  \n",
       "OriginalOperator            NaN       NaN  \n",
       "OriginalWellName            NaN       NaN  \n",
       "ProducedPools               NaN       NaN  \n",
       "QQ                          NaN       NaN  \n",
       "Range                       NaN       NaN  \n",
       "Section                     0.0       0.0  \n",
       "TD                          0.0       0.0  \n",
       "Township                    NaN       NaN  \n",
       "WellStatus                  NaN       NaN  \n",
       "WellType                    NaN       NaN  \n",
       "Wellbore                    NaN       NaN  \n",
       "api                         NaN       NaN  \n",
       "bakken_isopach_ft           0.0       0.0  \n",
       "bh_lat                      0.0       0.0  \n",
       "bh_lng                   6196.0      94.9  \n",
       "choke_size                  NaN       NaN  \n",
       "legs                        0.0       0.0  \n",
       "max_tvd                     0.0       0.0  \n",
       "mean_tvd                    0.0       0.0  \n",
       "min_tvd                     0.0       0.0  \n",
       "num_pools_produced          0.0       0.0  \n",
       "production_liquid_120       0.0       0.0  \n",
       "production_liquid_150       0.0       0.0  \n",
       "production_liquid_180       0.0       0.0  \n",
       "production_liquid_1825      0.0       0.0  \n",
       "production_liquid_270       0.0       0.0  \n",
       "production_liquid_30        0.0       0.0  \n",
       "production_liquid_365       0.0       0.0  \n",
       "production_liquid_60        0.0       0.0  \n",
       "production_liquid_730       0.0       0.0  \n",
       "production_liquid_90        0.0       0.0  \n",
       "spud_date                   NaN       NaN  \n",
       "std_tvd                     0.0       0.0  \n",
       "stimulated_formation        NaN       NaN  \n",
       "surface_lat                 0.0       0.0  \n",
       "surface_lng              6529.0     100.0  \n",
       "total_lbs_proppant          0.0       0.0  \n",
       "total_num_stages            0.0       0.0  \n",
       "total_volume_bbls           0.0       0.0  \n",
       "tvd                         0.0       0.0  \n",
       "type_treatment              NaN       NaN  \n",
       "well_status_date            NaN       NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.get_bad_vals_summaries(train_df, train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def munge_pipe(df, blacklist_patterns=[], exceptions={}, null_cutoff=.05):\n",
    "    '''\n",
    "    parameters: dataframe, blacklist patterns (as list), exceptions to blacklist patterns\n",
    "        (as set)\n",
    "    returns: copy of munged dataframe\n",
    "    '''\n",
    "    print(f\"df shape before removals {df.shape}\")\n",
    "    \n",
    "    df = (df.copy()\n",
    "            .pipe(dmt.drop_blacklist, blacklist_patterns=blacklist_patterns, exceptions=exceptions)\n",
    "            .pipe(dmt.drop_high_cardinality, exceptions=exceptions)\n",
    "            .pipe(dmt.drop_high_nulls, exceptions=exceptions, cutoff=null_cutoff)\n",
    "            )\n",
    "    \n",
    "    print (f\"df shape after removals {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape before removals (6529, 53)\n",
      "Shape before cardinality removal: {}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'format'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ab246788f1ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmunged_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmunge_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblacklist_patterns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblacklist_patterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-aa6ecc24338a>\u001b[0m in \u001b[0;36mmunge_pipe\u001b[0;34m(df, blacklist_patterns, exceptions, null_cutoff)\u001b[0m\n\u001b[1;32m      9\u001b[0m     df = (df.copy()\n\u001b[1;32m     10\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_blacklist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblacklist_patterns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mblacklist_patterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_high_cardinality\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_high_nulls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutoff\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnull_cutoff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             )\n",
      "\u001b[0;32m~/miniconda3/envs/my_env/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mpipe\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   5027\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pipe\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_shared_doc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5028\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5029\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5031\u001b[0m     _shared_docs[\"aggregate\"] = dedent(\n",
      "\u001b[0;32m~/miniconda3/envs/my_env/lib/python3.7/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36m_pipe\u001b[0;34m(obj, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/data_science/public_projects/data_imputation/scripts/data_munging_tools.py\u001b[0m in \u001b[0;36mdrop_high_cardinality\u001b[0;34m(df, exceptions, id_col)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcardinality\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     '''\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape before cardinality removal: {}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'format'"
     ]
    }
   ],
   "source": [
    "munged_df = munge_pipe(train_df, blacklist_patterns=blacklist_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape before removals (6529, 48)\n",
      "Shape before blacklist removal: (6529, 48)\n",
      "Blacklisted columns: ['bakken_isopach_ft', 'production_liquid_120', 'production_liquid_150', 'production_liquid_180', 'production_liquid_1825', 'production_liquid_270', 'production_liquid_30', 'production_liquid_365', 'production_liquid_60', 'production_liquid_730', 'total_num_stages']\n",
      "Number of blacklisted columns: 11\n",
      "Shape after blacklist removal: (6529, 37)\n",
      "**************************************************\n",
      "Shape before cardinality removal: (6529, 37)\n",
      "Dropped CurrentWellName since it was categorical and had a high cardinality\n",
      "Dropped Footages since it was categorical and had a high cardinality\n",
      "Dropped LeaseName since it was categorical and had a high cardinality\n",
      "Dropped LeaseNumber since it was categorical and had a high cardinality\n",
      "Dropped OriginalWellName since it was categorical and had a high cardinality\n",
      "Shape after cardinality removal: (6529, 32)\n",
      "**************************************************\n",
      "Shape before high null removal: (6529, 32)\n",
      "Dropped DFElev since it had a high proportion of missing values. 0.999693674376\n",
      "Shape before high null removal: (6529, 31)\n",
      "df shape after removals (6529, 31)\n"
     ]
    }
   ],
   "source": [
    "train_df = munge_pipe(train_df, blacklist_patterns=my_blacklist_patterns, exceptions=set([TARGET_1]), null_cutoff=.18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape before removals (1586, 48)\n",
      "Shape before blacklist removal: (1586, 48)\n",
      "Blacklisted columns: ['bakken_isopach_ft', 'production_liquid_120', 'production_liquid_150', 'production_liquid_180', 'production_liquid_1825', 'production_liquid_270', 'production_liquid_30', 'production_liquid_365', 'production_liquid_60', 'production_liquid_730', 'total_num_stages']\n",
      "Number of blacklisted columns: 11\n",
      "Shape after blacklist removal: (1586, 37)\n",
      "**************************************************\n",
      "Shape before cardinality removal: (1586, 37)\n",
      "Dropped CurrentWellName since it was categorical and had a high cardinality\n",
      "Dropped DFElev since it was empty\n",
      "Dropped Footages since it was categorical and had a high cardinality\n",
      "Dropped LeaseName since it was categorical and had a high cardinality\n",
      "Dropped LeaseNumber since it was categorical and had a high cardinality\n",
      "Dropped OriginalWellName since it was categorical and had a high cardinality\n",
      "Shape after cardinality removal: (1586, 31)\n",
      "**************************************************\n",
      "Shape before high null removal: (1586, 31)\n",
      "Shape before high null removal: (1586, 31)\n",
      "df shape after removals (1586, 31)\n"
     ]
    }
   ],
   "source": [
    "test_df = munge_pipe(test_df, blacklist_patterns=my_blacklist_patterns, exceptions=set([TARGET_1]), null_cutoff=.18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6529, 31)\n"
     ]
    }
   ],
   "source": [
    "print train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1586, 31)\n"
     ]
    }
   ],
   "source": [
    "print test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[TARGET_1].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[TARGET_1].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build Models\n",
    "\n",
    "etr = ExtraTreesRegressor(n_estimators=TREE_COUNT, max_depth=MAX_DEPTH, n_jobs=-1)\n",
    "\n",
    "etr.fit(rejoined_train_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fancyimpute import BiScaler, KNN, NuclearNormMinimization, SoftImpute, SimpleFill, MICE, MatrixFactorization, IterativeSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#instantiate imputers:\n",
    "sf_median = SimpleFill(fill_method=\"median\")\n",
    "sf_mean = SimpleFill(fill_method=\"mean\")\n",
    "knn_imputer = KNN(k=5, verbose=0)\n",
    "mice_imputer = MICE(verbose=0, )\n",
    "mf_imputer = MatrixFactorization(verbose=0)\n",
    "soft_imputer = SoftImpute(verbose=0)\n",
    "svd_imputer = IterativeSVD\n",
    "nonnormed_imputers_dict = {\"sf_median\" : sf_median, \"sf_mean\" : sf_mean, \"knn_imputer\" : knn_imputer}\n",
    "imputers_dict = {\"sf_median\" : sf_median, \"sf_mean\" : sf_mean, \"knn_imputer\" : knn_imputer, \"mice_imputer\" : mice_imputer}\n",
    "all_imputers_dict = {\"sf_median\" : sf_median, \"sf_mean\" : sf_mean, \"knn_imputer\" : knn_imputer, \"mice_imputer\": mice_imputer, \"mf_imputer\": mf_imputer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fancy_impute_pipe(train_df, test_df, target, imputer):\n",
    "    \"\"\"\n",
    "    Parameters: training dataframe, testing dataframe, target variable name (as a string), imputer object\n",
    "    Returns: filled and binarized training dataframe, filled and binarized training dataframe\n",
    "    \"\"\"\n",
    "    test_df = test_df.copy()\n",
    "    train_df = train_df.copy()\n",
    "\n",
    "    # Drop rows with missing target values\n",
    "    test_df.dropna(subset=[target], inplace=True)\n",
    "    train_df.dropna(subset=[target], inplace=True)\n",
    "    test_df.reset_index(inplace=True)\n",
    "    train_df.reset_index(inplace=True)\n",
    "\n",
    "    #create flags for test and train\n",
    "    flag_test_train(train_df, test_df)\n",
    "\n",
    "    ### Split into X and y\n",
    "    X_train, y_train = X_y_split(train_df, target)\n",
    "    X_test, y_test = X_y_split(test_df, target)\n",
    "\n",
    "    #Merge train and test for binarization of train and test and imputation of test\n",
    "    merged_df = pd.concat([X_train, X_test])\n",
    "\n",
    "    #split into numeric and nonnumeric\n",
    "    numeric_df, nonnumeric_df = split_numerical_features(merged_df, verbose=0)\n",
    "    \n",
    "    \n",
    "    #Binarize nonnumeric features\n",
    "    binarized_df = pd.get_dummies(nonnumeric_df)\n",
    "\n",
    "    #resplit into train and test\n",
    "    numerics_train_df = numeric_df[numeric_df[\"flag\"] == 0]\n",
    "    numerics_test_df = numeric_df[numeric_df[\"flag\"] == 1]\n",
    "    binarized_train_df = binarized_df[binarized_df[\"flag_str_train\"] == 1]\n",
    "    binarized_test_df = binarized_df[binarized_df[\"flag_str_test\"] == 1]\n",
    "\n",
    "    #perform imputations\n",
    "    filled_train_df = fancy_impute(numerics_train_df, imputer)\n",
    "    filled_df = fancy_impute(numeric_df, imputer)\n",
    "    \n",
    "\n",
    "    #scaling and/or imputing creates rounding error\n",
    "    filled_df[\"flag\"] = filled_df[\"flag\"].round(0)\n",
    "\n",
    "    #separate imputed test set from imputed train set\n",
    "    filled_test_df = filled_df[filled_df[\"flag\"] == 1]\n",
    "    \n",
    "    #rejoin test and train\n",
    "    binarized_train_df.reset_index(inplace=True, drop=True)\n",
    "    binarized_test_df.reset_index(inplace=True, drop=True) \n",
    "    filled_train_df.reset_index(inplace=True, drop=True)\n",
    "    filled_test_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    \n",
    "    rejoined_train_df = filled_train_df.join(binarized_train_df)\n",
    "    rejoined_test_df = filled_test_df.join(binarized_test_df)\n",
    "    \n",
    "    print \"rejoined train\", short_info(rejoined_train_df), \"\\n\"\n",
    "    \n",
    "    print \"rejoined test\", short_info(rejoined_test_df)\n",
    "\n",
    "    return rejoined_train_df, rejoined_test_df, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rejoined train \n",
      "**************************************************\n",
      "dataframe name: []\n",
      "shape: (6396, 847)\n",
      "index: RangeIndex(start=0, stop=6396, step=1)\n",
      "Nulls exist: False\n",
      "None \n",
      "\n",
      "rejoined test \n",
      "**************************************************\n",
      "dataframe name: []\n",
      "shape: (1558, 847)\n",
      "index: RangeIndex(start=0, stop=1558, step=1)\n",
      "Nulls exist: False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "rejoined_train_df, rejoined_test_df, y_train, y_test = fancy_impute_pipe(train_df, test_df, TARGET_1, mice_imputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rejoined_train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60731483621468763"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etr.score(rejoined_test_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05,\n",
       " 'max_depth': 5,\n",
       " 'min_samples_split': 3,\n",
       " 'n_estimators': 1000,\n",
       " 'subsample': 0.6}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01,\n",
       " 'max_depth': 7,\n",
       " 'min_samples_split': 3,\n",
       " 'n_estimators': 2000,\n",
       " 'subsample': 0.7}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(learning_rate=0.01, n_estimators=2000, subsample = .7, max_depth =7, min_samples_split= 3, random_state=1984)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.01, loss='ls', max_depth=7, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=3, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=2000, presort='auto', random_state=1984,\n",
       "             subsample=0.7, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr.fit(rejoined_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6266726640775433"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr.score(rejoined_test_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61430816909047947"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr.score(rejoined_test_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gbr2 = GradientBoostingRegressor(random_state=1984)\n",
    "\n",
    "params = {\"learning_rate\": [.001, .01, .05], \"n_estimators\": [1000, 2000], \"max_depth\": [3, 5, 7], \"min_samples_split\": [3, 4], \"subsample\": [.5, .6, .7]}\n",
    "\n",
    "grid = GridSearchCV(estimator=gbr2,param_grid=params, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=1984,\n",
       "             subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [1000, 2000], 'min_samples_split': [3, 4], 'learning_rate': [0.001, 0.01, 0.05], 'max_depth': [3, 5, 7], 'subsample': [0.5, 0.6, 0.7]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(rejoined_train_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "etr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mft.eval_model(etr, X_test, y_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mft.eval_model(gbr, X_test, y_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_array = np.array(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [etr, gbr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mft.most_important_features(etr, feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mft.most_important_features(gbr, feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#whoops, index is still in there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from fancyimpute import BiScaler, SimpleFill\n",
    "import model_fitting_tools as mft\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def drop_high_cardinality(df, exceptions={}, id_col=\"\"):\n",
    "    '''\n",
    "    Drop cardinality == 0, cardinality == 1, cardinality == n,\n",
    "    or (type='categorical and cardinality > 0.2 * n)\n",
    "    '''\n",
    "    print (\"Shape before cardinality removal: {}\").format(df.shape)\n",
    "    for col in df.columns:\n",
    "        if col in exceptions:\n",
    "            continue\n",
    "        else:\n",
    "            if df[col].count() == 0:\n",
    "                # drop cardinality = 0 (empty columns)\n",
    "                df.drop(col, inplace=True, axis=1)\n",
    "                print 'Dropped {} since it was empty'.format(col)\n",
    "            elif df[col].count() == 1:\n",
    "                # drop cardinality = 1\n",
    "                df.drop(col, inplace=True, axis=1)\n",
    "                print 'Dropped {} since it was always the same'.format(col)\n",
    "            elif df[col].count() == df[col].value_counts().idxmax():\n",
    "                # drop cardinality == count\n",
    "                df.drop(col, inplace=True, axis=1)\n",
    "                print 'Dropped {} since it was always unique'.format(col)\n",
    "            elif col != id_col and df[col].dtype == 'object' and len(df[col].value_counts()) > len(df) * 0.2:\n",
    "                df.drop(col, inplace=True, axis=1)\n",
    "                print 'Dropped {} since it was categorical and had a high cardinality'.format(col)\n",
    "    print (\"Shape after cardinality removal: {}\").format(df.shape)\n",
    "\n",
    "def drop_high_nulls(df, exceptions={}, cutoff=0.5):\n",
    "    print (\"Shape before high null removal: {}\").format(df.shape)\n",
    "    for col in df.columns:\n",
    "        if col in exceptions:\n",
    "            continue\n",
    "        else:\n",
    "            prop_missing = df[col].isnull().sum() / float(df[col].shape[0])\n",
    "            if prop_missing > cutoff:\n",
    "                df.drop(col, inplace=True, axis=1)\n",
    "                print 'Dropped {} since it had a high proportion of missing values. {}'.format(col, prop_missing)\n",
    "    print (\"Shape before high null removal: {}\").format(df.shape)\n",
    "\n",
    "def drop_categorical_features (df):\n",
    "    print \"Shape before removal: {}\".format(df.shape)\n",
    "    columns_removed= []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtypes == object:\n",
    "            df.drop(col, inplace=True, axis=1)\n",
    "            columns_removed.append(col)\n",
    "    print \"Categorical olumns dropped: {}\".format(columns_removed)\n",
    "    print \"Shape after removal: {}\".format(df.shape)\n",
    "\n",
    "def drop_nonnumeric_features (df):\n",
    "    df = df.copy()\n",
    "    print \"Shape before removal: {}\".format(df.shape)\n",
    "    columns_removed= []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtypes != float and df[col].dtypes != int:\n",
    "            df.drop(col, inplace=True, axis=1)\n",
    "            columns_removed.append(col)\n",
    "    print \"Columns dropped: {}\".format(columns_removed)\n",
    "    print \"Shape after removal: {}\".format(df.shape)\n",
    "    return df\n",
    "\n",
    "def split_numerical_features(df, verbose=1):\n",
    "    numeric_cols = []\n",
    "    nonnumeric_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtypes == float or df[col].dtypes == int:\n",
    "            numeric_cols.append(col)\n",
    "        else:\n",
    "            nonnumeric_cols.append(col)\n",
    "    numeric_df = df[numeric_cols]\n",
    "    nonnumeric_df = df[nonnumeric_cols]\n",
    "    if verbose == 1:\n",
    "        print \"numeric columns: {}\".format(numeric_cols)\n",
    "        print \"non-numeric columns: {}\".format(nonnumeric_cols)\n",
    "    return numeric_df, nonnumeric_df\n",
    "\n",
    "def fancy_impute(df, imputer):\n",
    "    '''\n",
    "    fills numerical dataframe with fancy imputer and returns completed dataframe\n",
    "    '''\n",
    "    if type(imputer) != SimpleFill:\n",
    "\n",
    "        biscaler = BiScaler(verbose=0)\n",
    "    \n",
    "        normed = biscaler.fit_transform(df.as_matrix())\n",
    "\n",
    "        filled_mat = imputer.complete(normed)\n",
    "        filled_mat = biscaler.inverse_transform(filled_mat)\n",
    "\n",
    "    else:\n",
    "        filled_mat = imputer.complete(df)\n",
    "\n",
    "    filled_df = pd.DataFrame(filled_mat, columns= df.columns)\n",
    "\n",
    "    return filled_df\n",
    "\n",
    "def extra_fancy_impute(df, simple_imputer, fancy_imputer, important_features):\n",
    "    '''\n",
    "    first, fill all nulls on most features with a simple imputation method, like median().\n",
    "    second, fill remaining nulls on important features with fancy imputer.\n",
    "    '''\n",
    "    first_pass_df = df[df.columns.difference(important_features)]\n",
    "    first_pass_filled = simple_imputer.complete(first_pass_df)\n",
    "    second_pass = np.concatenate((first_pass_filled, df[important_features].as_matrix()), axis=1)\n",
    "    print first_pass_filled.shape, df[important_features].as_matrix().shape\n",
    "    print second_pass.shape\n",
    "    biscaler = BiScaler(verbose=0)\n",
    "    normed = biscaler.fit_transform(second_pass)\n",
    "    filled_mat = fancy_imputer.complete(normed)\n",
    "    filled_mat = biscaler.inverse_transform(filled_mat)\n",
    "    filled_df = pd.DataFrame(filled_mat, columns= df.columns)\n",
    "    return filled_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def flag_test_train(df_train, df_test, string_flag=True):\n",
    "    '''\n",
    "    #create two flags for test and train, where one flag is a string, the other is a binary\n",
    "    '''\n",
    "    df_train[\"flag\"] = 0\n",
    "    df_test[\"flag\"] = 1\n",
    "    if string_flag == True:\n",
    "        df_train[\"flag_str\"] = \"train\"\n",
    "        df_test[\"flag_str\"] = \"test\"\n",
    "        \n",
    "def X_y_split(df, target):\n",
    "    '''\n",
    "    params: df, target variable (as string),\n",
    "    returns: df_X, df_y\n",
    "    '''\n",
    "    df_y = df[target]\n",
    "    df_X = df.drop(target, axis=1)\n",
    "    return df_X, df_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
