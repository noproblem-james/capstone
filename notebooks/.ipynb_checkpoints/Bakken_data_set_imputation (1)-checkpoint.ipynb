{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('max_colwidth', 800)\n",
    "pd.set_option(\"display.max_rows\",101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data_munging_tools as dmt\n",
    "import eda_tools as et\n",
    "import impute_eval as ie\n",
    "import model_fitting_tools as mft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fancyimpute import BiScaler, KNN, NuclearNormMinimization, SoftImpute, SimpleFill, MICE, MatrixFactorization, IterativeSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testing_df = pd.read_csv('cleaned-input.test.tsv', sep='\\t', low_memory=False)\n",
    "training_df = pd.read_csv('cleaned-input.training.tsv', sep='\\t', low_memory=False)\n",
    "#df = pd.read_csv(\"stack_imputation_exp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "TREE_COUNT = 200\n",
    "MAX_DEPTH = 20\n",
    "#increase tree count to ~200\n",
    "\n",
    "TARGET_1 = 'production_liquid_90'\n",
    "TARGET_2 = 'production_liquid_180'\n",
    "TARGET_3 = 'production_liquid_365'\n",
    "\n",
    "etr = ExtraTreesRegressor(n_estimators=TREE_COUNT, max_depth=MAX_DEPTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#instantiate imputers:\n",
    "sf_median = SimpleFill(fill_method=\"median\")\n",
    "sf_mean = SimpleFill(fill_method=\"mean\")\n",
    "knn_imputer = KNN(k=10, verbose=0)\n",
    "mice_imputer = MICE(verbose=0)\n",
    "mf_imputer = MatrixFactorization(verbose=0)\n",
    "nnm_imputer = NuclearNormMinimization(verbose=0)\n",
    "soft_imputer = SoftImpute(verbose=0)\n",
    "svd_imputer = IterativeSVD(verbose=0)\n",
    "\n",
    "#create dicts of imputers\n",
    "nonnormed_imputers_dict = {\"sf_median\" : sf_median, \"sf_mean\" : sf_mean}\n",
    "imputers_dict = {\"sf_median\" : sf_median, \"sf_mean\" : sf_mean, \"knn_imputer\" : knn_imputer, \"mice_imputer\" : mice_imputer, \"soft_imputer\": soft_imputer}\n",
    "all_imputers_dict = {\"sf_median\" : sf_median, \"sf_mean\" : sf_mean, \"knn_imputer\" : knn_imputer, \"mice_imputer\": mice_imputer, \"mf_imputer\": mf_imputer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def short_info(df):\n",
    "    name =[x for x in globals() if globals()[x] is df][0]\n",
    "    print \"\\n\", \"*\"*50\n",
    "    print \"dataframe name: {}\".format(name)\n",
    "    print \"shape: {}\".format(df.shape)\n",
    "    print \"index: {}\".format(df.index)\n",
    "    print \"Nulls exist: {}\".format(np.any(df.isnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************************************************\n",
      "dataframe name: training_df\n",
      "shape: (6529, 53)\n",
      "index: RangeIndex(start=0, stop=6529, step=1)\n",
      "Nulls exist: True\n",
      "None \n",
      "**************************************************\n",
      "dataframe name: testing_df\n",
      "shape: (1586, 53)\n",
      "index: RangeIndex(start=0, stop=1586, step=1)\n",
      "Nulls exist: True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print short_info(training_df), short_info(testing_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_backup = df.copy()\n",
    "# df = df_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1586, 51)\n",
      "(6529, 51)\n"
     ]
    }
   ],
   "source": [
    "#dropping unique ID\n",
    "testing_df.drop([\"FileNo\", \"Section\"], axis=1, inplace=True)\n",
    "training_df.drop([\"FileNo\", \"Section\"], axis=1, inplace=True)\n",
    "print testing_df.shape\n",
    "print training_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_blacklist_patterns = ['^recent_ipt_', '^production_', 'total_num_stages', 'bakken_isopach_ft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountyName                0.000000\n",
       "CurrentOperator           0.000000\n",
       "CurrentWellName           0.000000\n",
       "DFElev                    1.000000\n",
       "FieldName                 0.000000\n",
       "Footages                  0.000000\n",
       "GRElev                    0.174023\n",
       "KBElev                    0.057377\n",
       "LeaseName                 0.000000\n",
       "LeaseNumber               0.000631\n",
       "OriginalOperator          0.000000\n",
       "OriginalWellName          0.000000\n",
       "ProducedPools             0.000631\n",
       "QQ                        0.000000\n",
       "Range                     0.000000\n",
       "TD                        0.000631\n",
       "Township                  0.000000\n",
       "WellStatus                0.000000\n",
       "WellType                  0.000000\n",
       "Wellbore                  0.000000\n",
       "api                       0.000000\n",
       "bakken_isopach_ft         0.000000\n",
       "bh_lat                    0.049811\n",
       "bh_lng                    0.049811\n",
       "choke_size                0.090794\n",
       "legs                      0.049811\n",
       "max_tvd                   0.049811\n",
       "mean_tvd                  0.049811\n",
       "min_tvd                   0.049811\n",
       "num_pools_produced        0.000000\n",
       "production_liquid_120     0.018285\n",
       "production_liquid_150     0.027112\n",
       "production_liquid_180     0.034048\n",
       "production_liquid_1825    0.904792\n",
       "production_liquid_270     0.090164\n",
       "production_liquid_30      0.015132\n",
       "production_liquid_365     0.170240\n",
       "production_liquid_60      0.016393\n",
       "production_liquid_730     0.442623\n",
       "production_liquid_90      0.017654\n",
       "spud_date                 0.006305\n",
       "std_tvd                   0.049811\n",
       "stimulated_formation      0.124212\n",
       "surface_lat               0.000000\n",
       "surface_lng               0.000000\n",
       "total_lbs_proppant        0.133039\n",
       "total_num_stages          0.000000\n",
       "total_volume_bbls         0.138083\n",
       "tvd                       0.049811\n",
       "type_treatment            0.170870\n",
       "well_status_date          0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_df.isnull().sum() / testing_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape before removals (1586, 51)\n",
      "Shape before blacklist removal: (1586, 51)\n",
      "Blacklisted columns: ['bakken_isopach_ft', 'production_liquid_120', 'production_liquid_150', 'production_liquid_180', 'production_liquid_1825', 'production_liquid_270', 'production_liquid_30', 'production_liquid_365', 'production_liquid_60', 'production_liquid_730', 'total_num_stages']\n",
      "Number of blacklisted columns: 11\n",
      "Shape after blacklist removal: (1586, 40)\n",
      "**************************************************\n",
      "Shape before cardinality removal: (1586, 40)\n",
      "Dropped CurrentWellName since it was categorical and had a high cardinality\n",
      "Dropped DFElev since it was empty\n",
      "Dropped Footages since it was categorical and had a high cardinality\n",
      "Dropped LeaseName since it was categorical and had a high cardinality\n",
      "Dropped LeaseNumber since it was categorical and had a high cardinality\n",
      "Dropped OriginalWellName since it was categorical and had a high cardinality\n",
      "Dropped api since it was categorical and had a high cardinality\n",
      "Dropped spud_date since it was categorical and had a high cardinality\n",
      "Dropped well_status_date since it was categorical and had a high cardinality\n",
      "Shape after cardinality removal: (1586, 31)\n",
      "**************************************************\n",
      "Shape before high null removal: (1586, 31)\n",
      "Shape before high null removal: (1586, 31)\n",
      "df shape after removals (1586, 31)\n",
      "\n",
      "****************************************************************************************************\n",
      "df shape before removals (6529, 51)\n",
      "Shape before blacklist removal: (6529, 51)\n",
      "Blacklisted columns: ['bakken_isopach_ft', 'production_liquid_120', 'production_liquid_150', 'production_liquid_180', 'production_liquid_1825', 'production_liquid_270', 'production_liquid_30', 'production_liquid_365', 'production_liquid_60', 'production_liquid_730', 'total_num_stages']\n",
      "Number of blacklisted columns: 11\n",
      "Shape after blacklist removal: (6529, 40)\n",
      "**************************************************\n",
      "Shape before cardinality removal: (6529, 40)\n",
      "Dropped CurrentWellName since it was categorical and had a high cardinality\n",
      "Dropped Footages since it was categorical and had a high cardinality\n",
      "Dropped LeaseName since it was categorical and had a high cardinality\n",
      "Dropped LeaseNumber since it was categorical and had a high cardinality\n",
      "Dropped OriginalWellName since it was categorical and had a high cardinality\n",
      "Dropped api since it was categorical and had a high cardinality\n",
      "Dropped spud_date since it was categorical and had a high cardinality\n",
      "Dropped well_status_date since it was categorical and had a high cardinality\n",
      "Shape after cardinality removal: (6529, 32)\n",
      "**************************************************\n",
      "Shape before high null removal: (6529, 32)\n",
      "Dropped DFElev since it had a high proportion of missing values. 0.999693674376\n",
      "Shape before high null removal: (6529, 31)\n",
      "df shape after removals (6529, 31)\n"
     ]
    }
   ],
   "source": [
    "target = TARGET_1\n",
    "\n",
    "test_df = dmt.munge_pipe(testing_df, exceptions=set([target]), blacklist_patterns=my_blacklist_patterns, null_cutoff=.18)\n",
    "\n",
    "print \"\\n\", \"*\"*100\n",
    "\n",
    "train_df = dmt.munge_pipe(training_df, exceptions=set([target]), blacklist_patterns=my_blacklist_patterns, null_cutoff=.18)\n",
    "\n",
    "# Dropping other unique identifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, a well dropped out? 79 -> 78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6529, 31)\n",
      "(871, 31)\n",
      "(5658, 31)\n",
      "**************************************************\n",
      "(1586, 31)\n",
      "(211, 31)\n",
      "(1375, 31)\n"
     ]
    }
   ],
   "source": [
    "print train_df.shape\n",
    "#defining groups where num_clusters is present or absent.\n",
    "train_df_no_val = train_df[train_df.total_lbs_proppant.isnull()]\n",
    "train_df_val = train_df[train_df.total_lbs_proppant.notnull()]\n",
    "print train_df_no_val.shape\n",
    "print train_df_val.shape\n",
    "# print df_no_clust.shape[0] + df_clust.shape[0] == train_df.shape[0]\n",
    "\n",
    "print \"*\"*50\n",
    "print test_df.shape\n",
    "#defining groups where num_clusters is present or absent.\n",
    "test_df_no_val = test_df[test_df.total_lbs_proppant.isnull()]\n",
    "test_df_val = test_df[test_df.total_lbs_proppant.notnull()]\n",
    "print test_df_no_val.shape\n",
    "print test_df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n",
      "116\n",
      "133\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "print train_df_no_val[TARGET_1].isnull().sum()\n",
    "print train_df_val[TARGET_1].isnull().sum()\n",
    "print train_df[TARGET_1].isnull().sum()\n",
    "print test_df[TARGET_1].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print df_no_clust[TARGET_2].isnull().sum()\n",
    "print df_clust[TARGET_2].isnull().sum()\n",
    "print train_df[TARGET_2].isnull().sum()\n",
    "print test_df[TARGET_2].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#randomizing, since we are adding rows n-at-a-time\n",
    "train_df_no_val = train_df_no_val.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "train_df_val.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding wells with missing num_clusters to the training set, 67 at a time, imputing and fitting model 50 times per imputation method, per partition of new wells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 13, 67]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#possible partitions = factors of total_num_rows, ie. df_no_clust.shape[0]\n",
    "#e.g., 78\n",
    "[num for num in range(1, 871) if 871 % num == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total rows with missing data:  871\n",
      "rows per partition:  67\n",
      " partitions:  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "fitting model with only rows where num_clusters exists.\n"
     ]
    }
   ],
   "source": [
    "bakken_big_test_90_200 = ie.add_observations_impute(train_df_val, train_df_no_val, test_df, TARGET_1, imputers_dict, etr, num_partitions=13, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bakken_big_test_90_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bakken_big_test_90_200_df = pd.DataFrame(bakken_big_test_90_200, columns=[\"num_rows\", \"imputer_type\", \"mae\"]).sort_values(\"num_rows\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bakken_big_test_90_200_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bakken_big_test_90_200_df.to_csv(\"bakken_big_test_90_200.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "results_df = bakken_big_test_90_200_df\n",
    "\n",
    "filtered_df = results_df[((results_df['imputer_type'] == \"sf_median\") | (results_df['imputer_type'] == \"knn_imputer\") | \\\n",
    "                          (results_df['imputer_type'] == \"mice_imputer\") | (results_df['imputer_type'] == \"soft_imputer\")) & (results_df['num_rows'] % 6 == 0)\n",
    "                            ]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# the size of A4 paper\n",
    "fig.set_size_inches(24, 9)\n",
    "\n",
    "sns.boxplot(x=\"num_rows\", y=\"mae\", hue=\"imputer_type\", data=filtered_df, palette=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Draw a nested boxplot to show bills by day and sex\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "results_df = big_test_90_200_df\n",
    "\n",
    "filtered_df = results_df[((results_df['imputer_type'] == \"sf_median\") | (results_df['imputer_type'] == \"sf_mean\") | (results_df['imputer_type'] == \"mice_imputer\") | (results_df['imputer_type'] == \"soft_imputer\")) #& (results_df['num_rows'] % 6 == 0)\n",
    "                            ]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# the size of A4 paper\n",
    "fig.set_size_inches(10, 6)\n",
    "\n",
    "sns.boxplot(x=\"num_rows\", y=\"mae\", hue=\"imputer_type\", data=filtered_df, palette=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
