{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "import sys\n",
    "sys.path.insert(1, '../scripts/')\n",
    "import data_munging_tools as dmt\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframes\n",
    "test_df = pd.read_csv('../data/cleaned-input.test.tsv', sep='\\t', low_memory=False)\n",
    "train_df = pd.read_csv('../data/cleaned-input.training.tsv', sep='\\t', low_memory=False)\n",
    "\n",
    "blacklist_patterns = ['^recent_ipt_', '^production_']\n",
    "\n",
    "test_df.shape\n",
    "\n",
    "whitelist = ['production_liquid_180']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_df.columns\n",
    "\n",
    "# utils.get_bad_vals_summaries(train_df, train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def munge_pipe(df, blacklist_patterns=[], exceptions={}, null_cutoff=.05):\n",
    "    '''\n",
    "    parameters: dataframe, blacklist patterns (as list), exceptions to blacklist patterns\n",
    "        (as set)\n",
    "    returns: copy of munged dataframe\n",
    "    '''\n",
    "    print(f\"df shape before removals {df.shape}\")\n",
    "    \n",
    "    df = (df.copy()\n",
    "            .pipe(dmt.drop_blacklist, blacklist_patterns=blacklist_patterns, exceptions=exceptions)\n",
    "            .pipe(dmt.drop_high_cardinality, exceptions=exceptions)\n",
    "            .pipe(dmt.drop_high_nulls, exceptions=exceptions, cutoff=null_cutoff)\n",
    "            )\n",
    "    \n",
    "    print (f\"df shape after removals {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape before removals (6529, 53)\n",
      "Number of columns dropped for blacklist_pattern: 0\n",
      "Shape before cardinality removal: (6529, 43)\n",
      "Shape after cardinality removal: (6529, 35)\n",
      "Shape before high null removal: (6529, 35)\n",
      "Shape before high null removal: (6529, 34)\n",
      "df shape after removals (6529, 34)\n"
     ]
    }
   ],
   "source": [
    "munged_df = munge_pipe(train_df, blacklist_patterns=blacklist_patterns, null_cutoff=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['FileNo', 'CountyName', 'CurrentOperator', 'FieldName', 'GRElev',\n",
       "       'KBElev', 'OriginalOperator', 'ProducedPools', 'QQ', 'Range', 'Section',\n",
       "       'TD', 'Township', 'WellStatus', 'WellType', 'Wellbore', 'bh_lat',\n",
       "       'bh_lng', 'choke_size', 'legs', 'max_tvd', 'mean_tvd', 'min_tvd',\n",
       "       'num_pools_produced', 'std_tvd', 'stimulated_formation', 'surface_lat',\n",
       "       'surface_lng', 'total_lbs_proppant', 'total_volume_bbls', 'tvd',\n",
       "       'type_treatment'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "munged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape before removals (6529, 48)\n",
      "Shape before blacklist removal: (6529, 48)\n",
      "Blacklisted columns: ['bakken_isopach_ft', 'production_liquid_120', 'production_liquid_150', 'production_liquid_180', 'production_liquid_1825', 'production_liquid_270', 'production_liquid_30', 'production_liquid_365', 'production_liquid_60', 'production_liquid_730', 'total_num_stages']\n",
      "Number of blacklisted columns: 11\n",
      "Shape after blacklist removal: (6529, 37)\n",
      "**************************************************\n",
      "Shape before cardinality removal: (6529, 37)\n",
      "Dropped CurrentWellName since it was categorical and had a high cardinality\n",
      "Dropped Footages since it was categorical and had a high cardinality\n",
      "Dropped LeaseName since it was categorical and had a high cardinality\n",
      "Dropped LeaseNumber since it was categorical and had a high cardinality\n",
      "Dropped OriginalWellName since it was categorical and had a high cardinality\n",
      "Shape after cardinality removal: (6529, 32)\n",
      "**************************************************\n",
      "Shape before high null removal: (6529, 32)\n",
      "Dropped DFElev since it had a high proportion of missing values. 0.999693674376\n",
      "Shape before high null removal: (6529, 31)\n",
      "df shape after removals (6529, 31)\n"
     ]
    }
   ],
   "source": [
    "train_df = munge_pipe(train_df, blacklist_patterns=my_blacklist_patterns, exceptions=set([TARGET_1]), null_cutoff=.18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape before removals (1586, 48)\n",
      "Shape before blacklist removal: (1586, 48)\n",
      "Blacklisted columns: ['bakken_isopach_ft', 'production_liquid_120', 'production_liquid_150', 'production_liquid_180', 'production_liquid_1825', 'production_liquid_270', 'production_liquid_30', 'production_liquid_365', 'production_liquid_60', 'production_liquid_730', 'total_num_stages']\n",
      "Number of blacklisted columns: 11\n",
      "Shape after blacklist removal: (1586, 37)\n",
      "**************************************************\n",
      "Shape before cardinality removal: (1586, 37)\n",
      "Dropped CurrentWellName since it was categorical and had a high cardinality\n",
      "Dropped DFElev since it was empty\n",
      "Dropped Footages since it was categorical and had a high cardinality\n",
      "Dropped LeaseName since it was categorical and had a high cardinality\n",
      "Dropped LeaseNumber since it was categorical and had a high cardinality\n",
      "Dropped OriginalWellName since it was categorical and had a high cardinality\n",
      "Shape after cardinality removal: (1586, 31)\n",
      "**************************************************\n",
      "Shape before high null removal: (1586, 31)\n",
      "Shape before high null removal: (1586, 31)\n",
      "df shape after removals (1586, 31)\n"
     ]
    }
   ],
   "source": [
    "test_df = munge_pipe(test_df, blacklist_patterns=my_blacklist_patterns, exceptions=set([TARGET_1]), null_cutoff=.18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6529, 31)\n"
     ]
    }
   ],
   "source": [
    "print train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1586, 31)\n"
     ]
    }
   ],
   "source": [
    "print test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[TARGET_1].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[TARGET_1].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build Models\n",
    "\n",
    "etr = ExtraTreesRegressor(n_estimators=TREE_COUNT, max_depth=MAX_DEPTH, n_jobs=-1)\n",
    "\n",
    "etr.fit(rejoined_train_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fancyimpute import BiScaler, KNN, NuclearNormMinimization, SoftImpute, SimpleFill, MICE, MatrixFactorization, IterativeSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#instantiate imputers:\n",
    "sf_median = SimpleFill(fill_method=\"median\")\n",
    "sf_mean = SimpleFill(fill_method=\"mean\")\n",
    "knn_imputer = KNN(k=5, verbose=0)\n",
    "mice_imputer = MICE(verbose=0, )\n",
    "mf_imputer = MatrixFactorization(verbose=0)\n",
    "soft_imputer = SoftImpute(verbose=0)\n",
    "svd_imputer = IterativeSVD\n",
    "nonnormed_imputers_dict = {\"sf_median\" : sf_median, \"sf_mean\" : sf_mean, \"knn_imputer\" : knn_imputer}\n",
    "imputers_dict = {\"sf_median\" : sf_median, \"sf_mean\" : sf_mean, \"knn_imputer\" : knn_imputer, \"mice_imputer\" : mice_imputer}\n",
    "all_imputers_dict = {\"sf_median\" : sf_median, \"sf_mean\" : sf_mean, \"knn_imputer\" : knn_imputer, \"mice_imputer\": mice_imputer, \"mf_imputer\": mf_imputer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fancy_impute_pipe(train_df, test_df, target, imputer):\n",
    "    \"\"\"\n",
    "    Parameters: training dataframe, testing dataframe, target variable name (as a string), imputer object\n",
    "    Returns: filled and binarized training dataframe, filled and binarized training dataframe\n",
    "    \"\"\"\n",
    "    test_df = test_df.copy()\n",
    "    train_df = train_df.copy()\n",
    "\n",
    "    # Drop rows with missing target values\n",
    "    test_df.dropna(subset=[target], inplace=True)\n",
    "    train_df.dropna(subset=[target], inplace=True)\n",
    "    test_df.reset_index(inplace=True)\n",
    "    train_df.reset_index(inplace=True)\n",
    "\n",
    "    #create flags for test and train\n",
    "    flag_test_train(train_df, test_df)\n",
    "\n",
    "    ### Split into X and y\n",
    "    X_train, y_train = X_y_split(train_df, target)\n",
    "    X_test, y_test = X_y_split(test_df, target)\n",
    "\n",
    "    #Merge train and test for binarization of train and test and imputation of test\n",
    "    merged_df = pd.concat([X_train, X_test])\n",
    "\n",
    "    #split into numeric and nonnumeric\n",
    "    numeric_df, nonnumeric_df = split_numerical_features(merged_df, verbose=0)\n",
    "    \n",
    "    \n",
    "    #Binarize nonnumeric features\n",
    "    binarized_df = pd.get_dummies(nonnumeric_df)\n",
    "\n",
    "    #resplit into train and test\n",
    "    numerics_train_df = numeric_df[numeric_df[\"flag\"] == 0]\n",
    "    numerics_test_df = numeric_df[numeric_df[\"flag\"] == 1]\n",
    "    binarized_train_df = binarized_df[binarized_df[\"flag_str_train\"] == 1]\n",
    "    binarized_test_df = binarized_df[binarized_df[\"flag_str_test\"] == 1]\n",
    "\n",
    "    #perform imputations\n",
    "    filled_train_df = fancy_impute(numerics_train_df, imputer)\n",
    "    filled_df = fancy_impute(numeric_df, imputer)\n",
    "    \n",
    "\n",
    "    #scaling and/or imputing creates rounding error\n",
    "    filled_df[\"flag\"] = filled_df[\"flag\"].round(0)\n",
    "\n",
    "    #separate imputed test set from imputed train set\n",
    "    filled_test_df = filled_df[filled_df[\"flag\"] == 1]\n",
    "    \n",
    "    #rejoin test and train\n",
    "    binarized_train_df.reset_index(inplace=True, drop=True)\n",
    "    binarized_test_df.reset_index(inplace=True, drop=True) \n",
    "    filled_train_df.reset_index(inplace=True, drop=True)\n",
    "    filled_test_df.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    \n",
    "    rejoined_train_df = filled_train_df.join(binarized_train_df)\n",
    "    rejoined_test_df = filled_test_df.join(binarized_test_df)\n",
    "    \n",
    "    print \"rejoined train\", short_info(rejoined_train_df), \"\\n\"\n",
    "    \n",
    "    print \"rejoined test\", short_info(rejoined_test_df)\n",
    "\n",
    "    return rejoined_train_df, rejoined_test_df, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rejoined train \n",
      "**************************************************\n",
      "dataframe name: []\n",
      "shape: (6396, 847)\n",
      "index: RangeIndex(start=0, stop=6396, step=1)\n",
      "Nulls exist: False\n",
      "None \n",
      "\n",
      "rejoined test \n",
      "**************************************************\n",
      "dataframe name: []\n",
      "shape: (1558, 847)\n",
      "index: RangeIndex(start=0, stop=1558, step=1)\n",
      "Nulls exist: False\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "rejoined_train_df, rejoined_test_df, y_train, y_test = fancy_impute_pipe(train_df, test_df, TARGET_1, mice_imputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rejoined_train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60731483621468763"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etr.score(rejoined_test_df, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05,\n",
       " 'max_depth': 5,\n",
       " 'min_samples_split': 3,\n",
       " 'n_estimators': 1000,\n",
       " 'subsample': 0.6}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.01,\n",
       " 'max_depth': 7,\n",
       " 'min_samples_split': 3,\n",
       " 'n_estimators': 2000,\n",
       " 'subsample': 0.7}"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor(learning_rate=0.01, n_estimators=2000, subsample = .7, max_depth =7, min_samples_split= 3, random_state=1984)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.01, loss='ls', max_depth=7, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=3, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=2000, presort='auto', random_state=1984,\n",
       "             subsample=0.7, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr.fit(rejoined_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6266726640775433"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr.score(rejoined_test_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61430816909047947"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbr.score(rejoined_test_df, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gbr2 = GradientBoostingRegressor(random_state=1984)\n",
    "\n",
    "params = {\"learning_rate\": [.001, .01, .05], \"n_estimators\": [1000, 2000], \"max_depth\": [3, 5, 7], \"min_samples_split\": [3, 4], \"subsample\": [.5, .6, .7]}\n",
    "\n",
    "grid = GridSearchCV(estimator=gbr2,param_grid=params, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=100, presort='auto', random_state=1984,\n",
       "             subsample=1.0, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'n_estimators': [1000, 2000], 'min_samples_split': [3, 4], 'learning_rate': [0.001, 0.01, 0.05], 'max_depth': [3, 5, 7], 'subsample': [0.5, 0.6, 0.7]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(rejoined_train_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "etr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mft.eval_model(etr, X_test, y_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mft.eval_model(gbr, X_test, y_test, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_array = np.array(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [etr, gbr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mft.most_important_features(etr, feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mft.most_important_features(gbr, feature_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#whoops, index is still in there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from fancyimpute import BiScaler, SimpleFill\n",
    "import model_fitting_tools as mft\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def drop_high_cardinality(df, exceptions={}, id_col=\"\"):\n",
    "    '''\n",
    "    Drop cardinality == 0, cardinality == 1, cardinality == n,\n",
    "    or (type='categorical and cardinality > 0.2 * n)\n",
    "    '''\n",
    "    print (\"Shape before cardinality removal: {}\").format(df.shape)\n",
    "    for col in df.columns:\n",
    "        if col in exceptions:\n",
    "            continue\n",
    "        else:\n",
    "            if df[col].count() == 0:\n",
    "                # drop cardinality = 0 (empty columns)\n",
    "                df.drop(col, inplace=True, axis=1)\n",
    "                print 'Dropped {} since it was empty'.format(col)\n",
    "            elif df[col].count() == 1:\n",
    "                # drop cardinality = 1\n",
    "                df.drop(col, inplace=True, axis=1)\n",
    "                print 'Dropped {} since it was always the same'.format(col)\n",
    "            elif df[col].count() == df[col].value_counts().idxmax():\n",
    "                # drop cardinality == count\n",
    "                df.drop(col, inplace=True, axis=1)\n",
    "                print 'Dropped {} since it was always unique'.format(col)\n",
    "            elif col != id_col and df[col].dtype == 'object' and len(df[col].value_counts()) > len(df) * 0.2:\n",
    "                df.drop(col, inplace=True, axis=1)\n",
    "                print 'Dropped {} since it was categorical and had a high cardinality'.format(col)\n",
    "    print (\"Shape after cardinality removal: {}\").format(df.shape)\n",
    "\n",
    "def drop_high_nulls(df, exceptions={}, cutoff=0.5):\n",
    "    print (\"Shape before high null removal: {}\").format(df.shape)\n",
    "    for col in df.columns:\n",
    "        if col in exceptions:\n",
    "            continue\n",
    "        else:\n",
    "            prop_missing = df[col].isnull().sum() / float(df[col].shape[0])\n",
    "            if prop_missing > cutoff:\n",
    "                df.drop(col, inplace=True, axis=1)\n",
    "                print 'Dropped {} since it had a high proportion of missing values. {}'.format(col, prop_missing)\n",
    "    print (\"Shape before high null removal: {}\").format(df.shape)\n",
    "\n",
    "def drop_categorical_features (df):\n",
    "    print \"Shape before removal: {}\".format(df.shape)\n",
    "    columns_removed= []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtypes == object:\n",
    "            df.drop(col, inplace=True, axis=1)\n",
    "            columns_removed.append(col)\n",
    "    print \"Categorical olumns dropped: {}\".format(columns_removed)\n",
    "    print \"Shape after removal: {}\".format(df.shape)\n",
    "\n",
    "def drop_nonnumeric_features (df):\n",
    "    df = df.copy()\n",
    "    print \"Shape before removal: {}\".format(df.shape)\n",
    "    columns_removed= []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtypes != float and df[col].dtypes != int:\n",
    "            df.drop(col, inplace=True, axis=1)\n",
    "            columns_removed.append(col)\n",
    "    print \"Columns dropped: {}\".format(columns_removed)\n",
    "    print \"Shape after removal: {}\".format(df.shape)\n",
    "    return df\n",
    "\n",
    "def split_numerical_features(df, verbose=1):\n",
    "    numeric_cols = []\n",
    "    nonnumeric_cols = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtypes == float or df[col].dtypes == int:\n",
    "            numeric_cols.append(col)\n",
    "        else:\n",
    "            nonnumeric_cols.append(col)\n",
    "    numeric_df = df[numeric_cols]\n",
    "    nonnumeric_df = df[nonnumeric_cols]\n",
    "    if verbose == 1:\n",
    "        print \"numeric columns: {}\".format(numeric_cols)\n",
    "        print \"non-numeric columns: {}\".format(nonnumeric_cols)\n",
    "    return numeric_df, nonnumeric_df\n",
    "\n",
    "def fancy_impute(df, imputer):\n",
    "    '''\n",
    "    fills numerical dataframe with fancy imputer and returns completed dataframe\n",
    "    '''\n",
    "    if type(imputer) != SimpleFill:\n",
    "\n",
    "        biscaler = BiScaler(verbose=0)\n",
    "    \n",
    "        normed = biscaler.fit_transform(df.as_matrix())\n",
    "\n",
    "        filled_mat = imputer.complete(normed)\n",
    "        filled_mat = biscaler.inverse_transform(filled_mat)\n",
    "\n",
    "    else:\n",
    "        filled_mat = imputer.complete(df)\n",
    "\n",
    "    filled_df = pd.DataFrame(filled_mat, columns= df.columns)\n",
    "\n",
    "    return filled_df\n",
    "\n",
    "def extra_fancy_impute(df, simple_imputer, fancy_imputer, important_features):\n",
    "    '''\n",
    "    first, fill all nulls on most features with a simple imputation method, like median().\n",
    "    second, fill remaining nulls on important features with fancy imputer.\n",
    "    '''\n",
    "    first_pass_df = df[df.columns.difference(important_features)]\n",
    "    first_pass_filled = simple_imputer.complete(first_pass_df)\n",
    "    second_pass = np.concatenate((first_pass_filled, df[important_features].as_matrix()), axis=1)\n",
    "    print first_pass_filled.shape, df[important_features].as_matrix().shape\n",
    "    print second_pass.shape\n",
    "    biscaler = BiScaler(verbose=0)\n",
    "    normed = biscaler.fit_transform(second_pass)\n",
    "    filled_mat = fancy_imputer.complete(normed)\n",
    "    filled_mat = biscaler.inverse_transform(filled_mat)\n",
    "    filled_df = pd.DataFrame(filled_mat, columns= df.columns)\n",
    "    return filled_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def flag_test_train(df_train, df_test, string_flag=True):\n",
    "    '''\n",
    "    #create two flags for test and train, where one flag is a string, the other is a binary\n",
    "    '''\n",
    "    df_train[\"flag\"] = 0\n",
    "    df_test[\"flag\"] = 1\n",
    "    if string_flag == True:\n",
    "        df_train[\"flag_str\"] = \"train\"\n",
    "        df_test[\"flag_str\"] = \"test\"\n",
    "        \n",
    "def X_y_split(df, target):\n",
    "    '''\n",
    "    params: df, target variable (as string),\n",
    "    returns: df_X, df_y\n",
    "    '''\n",
    "    df_y = df[target]\n",
    "    df_X = df.drop(target, axis=1)\n",
    "    return df_X, df_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
